{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.loss import MeanSquaredError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from math import log\n",
    "\n",
    "def categorical_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true: the current predicted output of the model\n",
    "    :type y_true: np.ndarray\n",
    "    :param y_pred: the expected output\n",
    "    :type y_rped: np.ndarray\n",
    "    :return: the categorical-crossentropy-loss\n",
    "    :rtype: np.float64\n",
    "    This function calculates and return the categorical-crossentropy-loss.\n",
    "    \"+1e-15\" is just for adding a very small number to avoid np.log(0)\n",
    "    \"\"\"\n",
    "    loss = -np.sum(y_true * (np.log(y_pred+1e-15)))\n",
    "    return loss / (len(y_true))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769391936907927"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "c = categorical_crossentropy_loss(y_true, y_pred)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05, 0.95, 0.  ],\n",
       "       [0.1 , 0.8 , 0.1 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.mnist import MNIST\n",
    "\n",
    "from nn.model import Sequential\n",
    "\n",
    "from nn.activations import relu, tanh, sigmoid, softmax, leaky_relu\n",
    "from nn.layers import Dense\n",
    "from nn.loss import MeanSquaredError, CategoricalCrossEntropy\n",
    "from nn.optimizer import GradientDescent\n",
    "from nn.preprocessing import categorical_encoding, transform_input_data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|                model: Sequential                |\n",
      "+-------+---------------+------------+------------+\n",
      "| Layer | Weights shape | Bias shape | activation |\n",
      "+-------+---------------+------------+------------+\n",
      "| Dense |   (32, 784)   | <(32, 1)>  | leaky_relu |\n",
      "| Dense |    (10, 32)   | <(10, 1)>  |  softmax   |\n",
      "+-------+---------------+------------+------------+\n",
      "Optimizer: GradientDescent\n",
      "loss: categorical_cross_entropy\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential(input_shape=(1, 784),\n",
    "                layers=[\n",
    "                    Dense(32, activation=leaky_relu),\n",
    "                    Dense(10, activation='softmax')\n",
    "\n",
    "                ],\n",
    "                optimizer=GradientDescent(0.1),\n",
    "                loss=CategoricalCrossEntropy())\n",
    "\n",
    "nn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dense', '(32,784)', '<(32,', '1)>', 'leaky_relu']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+-----------------+\n",
      "| City name |  Area  | Population | Annual Rainfall |\n",
      "+-----------+--------+------------+-----------------+\n",
      "|   Moscow  | 123123 |   123123   |      12312      |\n",
      "+-----------+--------+------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"City name\", \"Area\", \"Population\", \"Annual Rainfall\"]\n",
    "x.add_row(['Moscow', 123123, 123123, 12312])\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kuweg\\VSCode\\purenn\\classification_test.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kuweg/VSCode/purenn/classification_test.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mprettytable\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required files are exist!\n"
     ]
    }
   ],
   "source": [
    "data = MNIST(mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10000, 784) (10000,)\n",
      "Test: (10000, 784) (10000,)\n",
      "(10000,)\n",
      "Training shapes\n",
      "X_train: (10000, 784, 1)\n",
      "y_train: (10000, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, test_data = data.dataset\n",
    "X_train, y_train = test_data\n",
    "X_test, y_test = test_data\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test_ = scaler.transform(X_test)\n",
    "\n",
    "X_test_ = transform_input_data(X_test_)\n",
    "X_train_ = transform_input_data(X_train)\n",
    "\n",
    "y_train_ = categorical_encoding(y_train)\n",
    "y_train_ = transform_input_data(y_train_)\n",
    "\n",
    "print('Training shapes')\n",
    "print('X_train:', X_train_.shape)\n",
    "print('y_train:', y_train_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "model: Sequential\n",
      "------------------------------\n",
      "Dense | (32,784) <(32, 1)> | leaky_relu\n",
      "Dense | (10,32) <(10, 1)> | softmax\n",
      "Optimizer: GradientDescent\n",
      "loss: categorical_cross_entropy\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential(input_shape=(1, 784),\n",
    "                layers=[\n",
    "                    Dense(32, activation=leaky_relu),\n",
    "                    Dense(10, activation='softmax')\n",
    "\n",
    "                ],\n",
    "                optimizer=GradientDescent(0.1),\n",
    "                loss=CategoricalCrossEntropy())\n",
    "\n",
    "nn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(32,784)', '<(32,', '1)>', 'leaky_relu']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= '(32,784) <(32, 1)> leaky_relu'\n",
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(10,32)', '<(10,', '1)>', 'softmax', 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = repr(nn.weights.wl1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07590196],\n",
       "       [0.09019457],\n",
       "       [0.08169743],\n",
       "       [0.09542794],\n",
       "       [0.18013433],\n",
       "       [0.11897169],\n",
       "       [0.08444522],\n",
       "       [0.07208916],\n",
       "       [0.11108785],\n",
       "       [0.09004985]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward(X_train_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5580\n",
      "<class 'list'> and <class 'list'> types as input data\n",
      "Start training for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 500.75samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cross_entropy: 0.2428749593210595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 500.04samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cross_entropy: 0.24908184467572342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = np.random.choice(range(len(X_train_)))\n",
    "print(n)\n",
    "nn.fit([X_train_[n]], [y_train_[n]], epochs=2)\n",
    "\n",
    "\n",
    "\n",
    "# sus = X_train_[n]\n",
    "# nn.forward(sus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> and <class 'numpy.ndarray'> types as input data\n",
      "Start training for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10000/10000 [00:13<00:00, 730.51samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cross_entropy: 2.302585092994046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10000/10000 [00:11<00:00, 858.16samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cross_entropy: 2.302585092994046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train_, y_train_, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.81502602e-03, 9.94305691e-03, 9.89621879e-03, ...,\n",
       "        2.88772962e-03, 3.14301970e-04, 3.02444976e-03],\n",
       "       [3.22840797e-03, 8.63284976e-04, 6.54818592e-03, ...,\n",
       "        5.33812051e-03, 2.38037526e-03, 8.17671149e-03],\n",
       "       [1.20525487e-03, 1.86226016e-03, 1.80422846e-03, ...,\n",
       "        5.34486227e-03, 5.24268917e-03, 7.11610660e-03],\n",
       "       ...,\n",
       "       [6.63143676e-03, 4.19025945e-03, 7.57930124e-03, ...,\n",
       "        1.16471069e-03, 7.69609734e-03, 5.85264882e-04],\n",
       "       [4.63019200e-03, 2.47159720e-05, 5.80103808e-03, ...,\n",
       "        3.67659457e-03, 8.44637804e-03, 1.84509258e-04],\n",
       "       [6.11765193e-03, 8.63943454e-03, 3.40758718e-03, ...,\n",
       "        1.54614323e-03, 7.16398836e-03, 6.21395014e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights.wl0.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ = nn.predict(X_train_[45])\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_train_[1]\n",
    "a = np.zeros(3)\n",
    "b = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.97418245481472"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0)/np.sum(np.exp([-2.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb3e545769a10a7f630e8bb60db30ddf996ef81130dc340db01e68062c397a14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
